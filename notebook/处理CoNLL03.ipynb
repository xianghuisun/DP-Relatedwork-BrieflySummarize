{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9aa369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3030134",
   "metadata": {},
   "source": [
    "# CoNLL03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1edc00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder='/home/xhsun/Desktop/gitRepositories/Some-NER-models/data/CoNLL03/en_conll03/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf8402",
   "metadata": {},
   "source": [
    "# 使用Spacy解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ec4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_sentence(sentence_list):\n",
    "    sentence=' '.join(sentence_list)\n",
    "    doc=nlp(sentence)\n",
    "    result=[]\n",
    "    for token in doc:\n",
    "        word=token.text\n",
    "        deprel=token.dep_.lower()\n",
    "        head=token.head.text\n",
    "        pos_tag=token.pos_\n",
    "        try:\n",
    "            head_id=sentence_list.index(str(head))\n",
    "            #容易出现parsing错误的情况\n",
    "        except:\n",
    "            return None\n",
    "        if deprel=='root':\n",
    "            head_id=0\n",
    "        else:\n",
    "            head_id+=1\n",
    "        result.append([word,pos_tag,head_id,deprel])\n",
    "    return result\n",
    "\n",
    "def process_data(path_folder,write_path):\n",
    "    files_path=os.listdir(path_folder)\n",
    "    print(files_path)\n",
    "    for file_name in files_path:\n",
    "        with open(os.path.join(path_folder,file_name)) as f:\n",
    "            lines=f.readlines()\n",
    "        sentences_and_entlabels=[([],[])]\n",
    "        for line in lines:\n",
    "            if line.strip() in ['',' ']:\n",
    "                sentences_and_entlabels.append(([],[]))\n",
    "            else:\n",
    "                line_split=line.strip().split()\n",
    "                assert len(line_split)==2\n",
    "                word,entity_label=line_split\n",
    "                sentences_and_entlabels[-1][0].append(word)\n",
    "                sentences_and_entlabels[-1][1].append(entity_label)\n",
    "                \n",
    "        if sentences_and_entlabels[-1]==([],[]):\n",
    "            del sentences_and_entlabels[-1]\n",
    "            \n",
    "        with open(os.path.join(write_path,file_name+\".conllx\"),'w') as f:\n",
    "            parsing_error_count=0\n",
    "            for example in tqdm(sentences_and_entlabels):\n",
    "                sentences,entlabels=example\n",
    "                assert len(sentences)==len(entlabels)\n",
    "                \n",
    "                parsing_result=parsing_sentence(sentence_list=sentences)\n",
    "                if parsing_result==None or len(parsing_result)!=len(entlabels):\n",
    "                    #parsing出现错误\n",
    "                    parsing_error_count+=1\n",
    "                    continue\n",
    "                    \n",
    "                for i in range(len(parsing_result)):\n",
    "                    word,pos_tag,head_id,deprel=parsing_result[i]\n",
    "                    ent=entlabels[i]\n",
    "                    lemma='_'\n",
    "                    feats='_'\n",
    "                    conllx_example=[str(i+1),word,lemma,pos_tag,pos_tag,feats,str(head_id),deprel,'_','_',ent]\n",
    "                    f.write('\\t'.join(conllx_example)+'\\n')\n",
    "                    \n",
    "                f.write('\\n')\n",
    "            print(\"parsing error count : \",parsing_error_count,len(sentences_and_entlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45c8c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.word.bmes', 'test.word.bmes', 'dev.word.bmes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 14041/14041 [00:53<00:00, 260.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  4026 14041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 3453/3453 [00:13<00:00, 256.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  979 3453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 3250/3250 [00:13<00:00, 249.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  1030 3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_data(path_folder=path_folder,write_path='/home/xhsun/Desktop/gitRepositories/DP-Relatedwork-BrieflySummarize/data/CoNLL03/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80853096",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Brussels 1996-08-22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b81e1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brussels Brussels ROOT\n",
      "1996 Brussels nummod\n",
      "- Brussels punct\n",
      "08 22 nummod\n",
      "- 22 punct\n",
      "22 Brussels appos\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,token.head,token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96e271",
   "metadata": {},
   "source": [
    "# 使用Stanza解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c536e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 19:03:33 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-01-02 19:03:33 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-02 19:03:33 INFO: Use device: gpu\n",
      "2022-01-02 19:03:33 INFO: Loading: tokenize\n",
      "2022-01-02 19:03:35 INFO: Loading: pos\n",
      "2022-01-02 19:03:35 INFO: Loading: lemma\n",
      "2022-01-02 19:03:35 INFO: Loading: depparse\n",
      "2022-01-02 19:03:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline('en', processors = 'tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8de0ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_sentence(sentence_list):\n",
    "    sentence=' '.join(sentence_list)\n",
    "    doc=nlp(sentence)\n",
    "    result=[]\n",
    "    for sent_dict in doc.sentences:\n",
    "        sent_dict=sent_dict.to_dict()\n",
    "        for each_dict in sent_dict:\n",
    "            id_=str(each_dict['id'])\n",
    "            word=each_dict['text']\n",
    "            lemma='_'\n",
    "            pos_tag=each_dict['upos']\n",
    "            cpostag=each_dict['upos']\n",
    "            feats='_'\n",
    "            head_id=str(each_dict['head'])\n",
    "            deprel=each_dict['deprel']\n",
    "\n",
    "            result.append([id_,word,lemma,pos_tag,pos_tag,feats,head_id,deprel,'_','_'])\n",
    "    return result\n",
    "\n",
    "def process_data(path_folder,write_path):\n",
    "    files_path=os.listdir(path_folder)\n",
    "    print(files_path)\n",
    "    for file_name in files_path:\n",
    "        with open(os.path.join(path_folder,file_name)) as f:\n",
    "            lines=f.readlines()\n",
    "        sentences_and_entlabels=[([],[])]\n",
    "        for line in lines:\n",
    "            if line.strip() in ['',' ']:\n",
    "                sentences_and_entlabels.append(([],[]))\n",
    "            else:\n",
    "                line_split=line.strip().split()\n",
    "                assert len(line_split)==2\n",
    "                word,entity_label=line_split\n",
    "                sentences_and_entlabels[-1][0].append(word)\n",
    "                sentences_and_entlabels[-1][1].append(entity_label)\n",
    "                \n",
    "        if sentences_and_entlabels[-1]==([],[]):\n",
    "            del sentences_and_entlabels[-1]\n",
    "            \n",
    "        with open(os.path.join(write_path,file_name+\".conllx\"),'w') as f:\n",
    "            parsing_error_count=0\n",
    "            for example in tqdm(sentences_and_entlabels):\n",
    "                sentences,entlabels=example\n",
    "                assert len(sentences)==len(entlabels)\n",
    "                \n",
    "                parsing_result=parsing_sentence(sentence_list=sentences)\n",
    "                if len(parsing_result)!=len(entlabels):\n",
    "                    #parsing出现错误\n",
    "                    parsing_error_count+=1\n",
    "                    continue\n",
    "                    \n",
    "                for i in range(len(parsing_result)):\n",
    "                    parsing_result[i].append(entlabels[i])\n",
    "                    f.write('\\t'.join(parsing_result[i])+'\\n')\n",
    "                    \n",
    "                f.write('\\n')\n",
    "            print(\"parsing error count : \",parsing_error_count,len(sentences_and_entlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63c365d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                    | 3/14041 [00:00<07:59, 29.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.word.bmes', 'test.word.bmes', 'dev.word.bmes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14041/14041 [07:26<00:00, 31.45it/s]\n",
      "  0%|▏                                                                                                                                                                    | 5/3453 [00:00<01:25, 40.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  2455 14041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3453/3453 [01:26<00:00, 39.79it/s]\n",
      "  0%|▏                                                                                                                                                                    | 4/3250 [00:00<01:46, 30.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  623 3453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3250/3250 [01:23<00:00, 38.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  698 3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_data(path_folder=path_folder,\n",
    "             write_path='/home/xhsun/Desktop/gitRepositories/Some-NER-models/data/CoNLL03/Stanza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9391dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11586"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14041-2455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f9a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f21e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb3906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.word.bmes', 'test.word.bmes', 'dev.word.bmes']\n"
     ]
    }
   ],
   "source": [
    "files_path=os.listdir(path_folder)\n",
    "print(files_path)\n",
    "for file_name in files_path:\n",
    "    with open(os.path.join(path_folder,file_name)) as f:\n",
    "        lines=f.readlines()\n",
    "    sentences_and_entlabels=[([],[])]\n",
    "    for line in lines:\n",
    "        if line.strip() in ['',' ']:\n",
    "            sentences_and_entlabels.append(([],[]))\n",
    "        else:\n",
    "            line_split=line.strip().split()\n",
    "            assert len(line_split)==2\n",
    "            word,entity_label=line_split\n",
    "            sentences_and_entlabels[-1][0].append(word)\n",
    "            sentences_and_entlabels[-1][1].append(entity_label)\n",
    "\n",
    "    if sentences_and_entlabels[-1]==([],[]):\n",
    "        del sentences_and_entlabels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ee3272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['--', 'Dhaka', 'Newsroom', '880-2-506363'], ['O', 'B-ORG', 'E-ORG', 'O'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_and_entlabels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773c9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Dhaka Newsroom 880-2-506363\n"
     ]
    }
   ],
   "source": [
    "sentence_list=sentences_and_entlabels[-1][0]\n",
    "sentence=' '.join(sentence_list)\n",
    "doc=nlp(sentence)\n",
    "result=[]\n",
    "print(sentence)\n",
    "for sent_dict in doc.sentences:\n",
    "    sent_dict=sent_dict.to_dict()\n",
    "    for each_dict in sent_dict:\n",
    "        id_=str(each_dict['id'])\n",
    "        word=each_dict['text']\n",
    "        lemma='_'\n",
    "        pos_tag=each_dict['upos']\n",
    "        cpostag=each_dict['upos']\n",
    "        feats='_'\n",
    "        head_id=str(each_dict['head'])\n",
    "        deprel=each_dict['deprel']\n",
    "\n",
    "        result.append([id_,word,lemma,pos_tag,pos_tag,feats,head_id,deprel,'_','_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0729ff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '--', '_', 'PUNCT', 'PUNCT', '_', '3', 'punct', '_', '_'],\n",
       " ['2', 'Dhaka', '_', 'PROPN', 'PROPN', '_', '3', 'compound', '_', '_'],\n",
       " ['3', 'Newsroom', '_', 'PROPN', 'PROPN', '_', '0', 'root', '_', '_'],\n",
       " ['4', '880-2-506363', '_', 'NUM', 'NUM', '_', '3', 'appos', '_', '_']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c2cae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51de94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c1a669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--', 'Dhaka', 'Newsroom', '880-2-506363']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5de164c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-ORG', 'E-ORG', 'O']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_and_entlabels[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b17e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
