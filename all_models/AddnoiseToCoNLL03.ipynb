{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9025dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b671c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder='/home/xhsun/Desktop/gitRepositories/Some-NER-models/data/CoNLL03/en_conll03/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ebe8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0fd8f",
   "metadata": {},
   "source": [
    "# 根据句子长度比例随机添加punckt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1cff9b",
   "metadata": {},
   "source": [
    "- 如果句子长度小于5,则不添加\n",
    "- 如果长度在6-10之间，随机添加1个\n",
    "- 如果长度在10-15之间，随机添加两个\n",
    "- 长度大于15的句子，随机添加3个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7dd28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts=[',','.','!','?','!!','??','#','\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fd56bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_sentence(sentence_list,ent_list):\n",
    "    sentence=' '.join(sentence_list)\n",
    "    doc=nlp(sentence)\n",
    "    result=[]\n",
    "    for i,token in enumerate(doc):\n",
    "        word=token.text\n",
    "        deprel=token.dep_.lower()\n",
    "        head=token.head.text\n",
    "        pos_tag=token.pos_\n",
    "        if head not in sentence_list:\n",
    "            #出现了parsing错误的问题，通常是因为一个词被分成了两个\n",
    "            if i>=len(ent_list):\n",
    "                i=len(ent_list)-1\n",
    "                \n",
    "            if head+sentence_list[i] in sentence_list:\n",
    "                head=head+sentence_list[i]\n",
    "            elif sentence_list[i-1]+head in sentence_list:\n",
    "                head=sentence_list[i-1]+head\n",
    "            else:\n",
    "                head=sentence_list[i]#实在找不到\n",
    "        head_id=sentence_list.index(str(head))\n",
    "        if deprel=='root':\n",
    "            head_id=0\n",
    "        else:\n",
    "            head_id+=1\n",
    "        if i>=len(ent_list):\n",
    "            i=len(ent_list)-1\n",
    "        result.append([word,pos_tag,head_id,deprel,ent_list[i]])\n",
    "    return result\n",
    "\n",
    "def process_data(path_folder,write_path):\n",
    "    files_path=os.listdir(path_folder)\n",
    "    print(files_path)\n",
    "    insert_sentence_count=0\n",
    "    for file_name in files_path:\n",
    "        with open(os.path.join(path_folder,file_name)) as f:\n",
    "            lines=f.readlines()\n",
    "        sentences_and_entlabels=[([],[])]\n",
    "        for line in lines:\n",
    "            if line.strip() in ['',' ']:\n",
    "                sentences_and_entlabels.append(([],[]))\n",
    "            else:\n",
    "                line_split=line.strip().split()\n",
    "                assert len(line_split)==2\n",
    "                word,entity_label=line_split\n",
    "                sentences_and_entlabels[-1][0].append(word)\n",
    "                sentences_and_entlabels[-1][1].append(entity_label)\n",
    "                \n",
    "        if sentences_and_entlabels[-1]==([],[]):\n",
    "            del sentences_and_entlabels[-1]\n",
    "            \n",
    "        for example in sentences_and_entlabels:\n",
    "            sentence_list,label_list=example\n",
    "            insert_=False\n",
    "            for _ in range(len(sentence_list)//5):\n",
    "                point=puncts[random.randint(a=0,b=len(puncts)-1)]\n",
    "                pos=random.randint(a=0,b=len(sentence_list)-1)\n",
    "                if label_list[pos]!='O':\n",
    "                    continue#只有在O的位置插入punckt才合理\n",
    "                else:\n",
    "                    sentence_list.insert(pos,point)\n",
    "                    label_list.insert(pos,'O')\n",
    "                    insert_=True\n",
    "                    \n",
    "            insert_sentence_count+=int(insert_)\n",
    "        print(insert_sentence_count)            \n",
    "        with open(os.path.join(write_path,file_name+\".conllx\"),'w') as f:\n",
    "            parsing_error_count=0\n",
    "            for example in tqdm(sentences_and_entlabels):\n",
    "                sentences,entlabels=example\n",
    "                assert len(sentences)==len(entlabels)\n",
    "                \n",
    "                parsing_result=parsing_sentence(sentence_list=sentences,ent_list=entlabels)\n",
    "#                 if parsing_result==None or len(parsing_result)!=len(entlabels):\n",
    "#                     #parsing出现错误\n",
    "#                     parsing_error_count+=1\n",
    "#                     continue\n",
    "                    \n",
    "                for i in range(len(parsing_result)):\n",
    "                    word,pos_tag,head_id,deprel,ent=parsing_result[i]\n",
    "                    #ent=entlabels[i]\n",
    "                    lemma='_'\n",
    "                    feats='_'\n",
    "                    conllx_example=[str(i+1),word,lemma,pos_tag,pos_tag,feats,str(head_id),deprel,'_','_',ent]\n",
    "                    f.write('\\t'.join(conllx_example)+'\\n')\n",
    "                    \n",
    "                f.write('\\n')\n",
    "            print(\"parsing error count : \",parsing_error_count,len(sentences_and_entlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dccac608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                            | 0/14041 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.word.bmes', 'test.word.bmes', 'dev.word.bmes']\n",
      "10099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14041/14041 [00:57<00:00, 246.24it/s]\n",
      "  1%|█                                                                                                                                                                  | 22/3453 [00:00<00:16, 211.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  0 14041\n",
      "12539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3453/3453 [00:13<00:00, 250.31it/s]\n",
      "  1%|▉                                                                                                                                                                  | 19/3250 [00:00<00:17, 188.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  0 3453\n",
      "14961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3250/3250 [00:13<00:00, 238.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing error count :  0 3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_data(path_folder=path_folder,write_path='/home/xhsun/Desktop/gitRepositories/Some-NER-models/data/NoiseCoNLL03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655f552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
