{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5226685b",
   "metadata": {},
   "source": [
    "# 使用Spacy工具包解析W-NUT17数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218da0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1db8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder='/home/xhsun/Desktop/gitRepositories/ADP2NER/data/W-NUT17/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34777217",
   "metadata": {},
   "source": [
    "# 使用Spacy解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f762b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(file_path):\n",
    "    with open(file_path) as f:\n",
    "        lines=f.readlines()\n",
    "    sentences_and_entlabels=[([],[])]\n",
    "    sen_lengths=0\n",
    "    for line in lines:\n",
    "        if line.strip() in ['',' ']:\n",
    "            sentences_and_entlabels.append(([],[]))\n",
    "        else:\n",
    "            line_split=line.strip().split()\n",
    "            assert len(line_split)==2\n",
    "            word,entity_label=line_split\n",
    "            sentences_and_entlabels[-1][0].append(word)\n",
    "            sentences_and_entlabels[-1][1].append(entity_label)\n",
    "\n",
    "    if sentences_and_entlabels[-1]==([],[]):\n",
    "        del sentences_and_entlabels[-1]\n",
    "    for example in sentences_and_entlabels:\n",
    "        sen_lengths+=len(example[0])\n",
    "    print(\"平均长度 : \",sen_lengths/len(sentences_and_entlabels))\n",
    "    return sentences_and_entlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2e22cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均长度 :  18.482616381850324\n",
      "平均长度 :  18.177156177156178\n"
     ]
    }
   ],
   "source": [
    "train_sentences_and_entlabels=get_all_sentences('/home/xhsun/Desktop/gitRepositories/ADP2NER/data/W-NUT17/train_prepro_url.txt')\n",
    "test_sentences_and_entlabels=get_all_sentences('/home/xhsun/Desktop/gitRepositories/ADP2NER/data/W-NUT17/test_prepro_url.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d625fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3394\n",
      "1287\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences_and_entlabels))\n",
    "print(len(test_sentences_and_entlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23d9013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location', 'O', 'B-location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences_and_entlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96d58854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@paulwalk It 's the view from where I 'm living for two weeks . Empire State Building = ESB . Pretty bad storm here last evening ."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(' '.join(train_sentences_and_entlabels[0][0]))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "717acc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 27 27\n"
     ]
    }
   ],
   "source": [
    "print(len(doc),len(train_sentences_and_entlabels[0][1]),len(train_sentences_and_entlabels[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1041e14",
   "metadata": {},
   "source": [
    "**会存在parsing错误导致id对不上的问题**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8466a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@paulwalk \t @paulwalk \t root \t PUNCT\n",
      "It \t 's \t nsubj \t PRON\n",
      "'s \t 's \t root \t AUX\n",
      "the \t view \t det \t DET\n",
      "view \t 's \t attr \t NOUN\n",
      "from \t view \t prep \t ADP\n",
      "where \t living \t advmod \t ADV\n",
      "I \t m \t poss \t PRON\n",
      "' \t I \t case \t PUNCT\n",
      "m \t living \t nsubj \t X\n",
      "living \t from \t pcomp \t VERB\n",
      "for \t living \t prep \t ADP\n",
      "two \t weeks \t nummod \t NUM\n",
      "weeks \t for \t pobj \t NOUN\n",
      ". \t 's \t punct \t PUNCT\n",
      "Empire \t ESB \t compound \t PROPN\n",
      "State \t ESB \t compound \t PROPN\n",
      "Building \t ESB \t nmod \t PROPN\n",
      "= \t ESB \t punct \t PUNCT\n",
      "ESB \t ESB \t root \t PROPN\n",
      ". \t ESB \t punct \t PUNCT\n",
      "Pretty \t bad \t advmod \t ADV\n",
      "bad \t storm \t amod \t ADJ\n",
      "storm \t storm \t root \t NOUN\n",
      "here \t storm \t advmod \t ADV\n",
      "last \t evening \t amod \t ADJ\n",
      "evening \t storm \t npadvmod \t NOUN\n",
      ". \t storm \t punct \t PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    word=token.text\n",
    "    deprel=token.dep_.lower()\n",
    "    head=token.head.text\n",
    "    pos_tag=token.pos_\n",
    "    print(word,'\\t',head,'\\t',deprel,'\\t',pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e23069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_sentence(sentence_list,ent_list):\n",
    "    sentence=' '.join(sentence_list)\n",
    "    doc=nlp(sentence)\n",
    "    result=[]\n",
    "    for i,token in enumerate(doc):\n",
    "        word=token.text\n",
    "        deprel=token.dep_.lower()\n",
    "        head=token.head.text\n",
    "        pos_tag=token.pos_\n",
    "        if head not in sentence_list:\n",
    "            #出现了parsing错误的问题，通常是因为一个词被分成了两个\n",
    "            if i>=len(ent_list):\n",
    "                i=len(ent_list)-1\n",
    "                \n",
    "            if head+sentence_list[i] in sentence_list:\n",
    "                head=head+sentence_list[i]\n",
    "            elif sentence_list[i-1]+head in sentence_list:\n",
    "                head=sentence_list[i-1]+head\n",
    "            else:\n",
    "                head=sentence_list[i]#实在找不到\n",
    "        head_id=sentence_list.index(str(head))\n",
    "        if deprel=='root':\n",
    "            head_id=0\n",
    "        else:\n",
    "            head_id+=1\n",
    "        if i>=len(ent_list):\n",
    "            i=len(ent_list)-1\n",
    "        result.append([word,pos_tag,head_id,deprel,ent_list[i]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "144a50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parsing_results(examples,write_path):\n",
    "    with open(write_path,'w') as f:\n",
    "        for example in tqdm(examples):\n",
    "            sentence_list,ent_list=example\n",
    "            parsing_result=parsing_sentence(sentence_list,ent_list)\n",
    "            for i in range(len(parsing_result)):\n",
    "                word,pos_tag,head_id,deprel,ent=parsing_result[i]\n",
    "                lemma='_'\n",
    "                feats='_'\n",
    "                conllx_example=[str(i+1),word,lemma,pos_tag,pos_tag,feats,str(head_id),deprel,'_','_',ent]\n",
    "                f.write('\\t'.join(conllx_example)+'\\n')\n",
    "            f.write('\\n')\n",
    "    print(\"write over!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(train_sentences_and_entlabels))\n",
    "print(len(test_sentences_and_entlabels))\n",
    "write_parsing_results(examples=train_sentences_and_entlabels,write_path='/home/xhsun/Desktop/gitRepositories/Some-NER-models/data/W-NUT17/Spacy/train.conllx')\n",
    "write_parsing_results(examples=test_sentences_and_entlabels,write_path='/home/xhsun/Desktop/gitRepositories/Some-NER-models/data/W-NUT17/Spacy/test.conllx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac694a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0746e677",
   "metadata": {},
   "source": [
    "# 使用Spacy工具包解析NCBI数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82267933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均长度 :  25.0186209439528\n",
      "平均长度 :  26.06063829787234\n"
     ]
    }
   ],
   "source": [
    "train_sentences_and_entlabels=get_all_sentences('/home/xhsun/Desktop/gitRepositories/ADP2NER/data/NCBI/train.txt')\n",
    "test_sentences_and_entlabels=get_all_sentences('/home/xhsun/Desktop/gitRepositories/ADP2NER/data/NCBI/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92a62c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5424\n",
      "940\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences_and_entlabels))\n",
    "print(len(test_sentences_and_entlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d18e49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Identification', 'of', 'APC2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'I-Disease', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences_and_entlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6abe97c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor ."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(' '.join(train_sentences_and_entlabels[0][0]))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585e8455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14 14\n"
     ]
    }
   ],
   "source": [
    "print(len(doc),len(train_sentences_and_entlabels[0][1]),len(train_sentences_and_entlabels[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c45e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                                                                  | 14/5424 [00:00<00:39, 136.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5424\n",
      "940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5424/5424 [00:28<00:00, 189.11it/s]\n",
      "  2%|███▏                                                                                                                                                                | 18/940 [00:00<00:05, 170.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write over!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 940/940 [00:05<00:00, 186.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write over!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_sentences_and_entlabels))\n",
    "print(len(test_sentences_and_entlabels))\n",
    "write_parsing_results(examples=train_sentences_and_entlabels,write_path='/home/xhsun/Desktop/gitRepositories/Some-NER-models/data/NCBI/Spacy/train.conllx')\n",
    "write_parsing_results(examples=test_sentences_and_entlabels,write_path='/home/xhsun/Desktop/gitRepositories/Some-NER-models/data/NCBI/Spacy/test.conllx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b8e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(file_path,write_path):\n",
    "    with open(file_path) as f:\n",
    "        lines=f.readlines()\n",
    "    sentences_and_entlabels=[([],[])]\n",
    "    for line in lines:\n",
    "        if line.strip() in ['',' ']:\n",
    "            sentences_and_entlabels.append(([],[]))\n",
    "        else:\n",
    "            line_split=line.strip().split()\n",
    "            assert len(line_split)==2\n",
    "            word,entity_label=line_split\n",
    "            sentences_and_entlabels[-1][0].append(word)\n",
    "            sentences_and_entlabels[-1][1].append(entity_label)\n",
    "\n",
    "    if sentences_and_entlabels[-1]==([],[]):\n",
    "        del sentences_and_entlabels[-1]\n",
    "\n",
    "    with open(write_path,'w') as f:\n",
    "        parsing_error_count=0\n",
    "        for example in tqdm(sentences_and_entlabels):\n",
    "            sentences,entlabels=example\n",
    "            assert len(sentences)==len(entlabels)\n",
    "\n",
    "            parsing_result=parsing_sentence(sentence_list=sentences,entlabels=entlabels)\n",
    "            if parsing_result==None or len(parsing_result)!=len(entlabels):\n",
    "                #parsing出现错误\n",
    "                parsing_error_count+=1\n",
    "                continue\n",
    "\n",
    "            for i in range(len(parsing_result)):\n",
    "                word,pos_tag,head_id,deprel=parsing_result[i]\n",
    "                ent=entlabels[i]\n",
    "                lemma='_'\n",
    "                feats='_'\n",
    "                conllx_example=[str(i+1),word,lemma,pos_tag,pos_tag,feats,str(head_id),deprel,'_','_',ent]\n",
    "                f.write('\\t'.join(conllx_example)+'\\n')\n",
    "\n",
    "            f.write('\\n')\n",
    "        print(\"parsing error count : \",parsing_error_count,len(sentences_and_entlabels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
